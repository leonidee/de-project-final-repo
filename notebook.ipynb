{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[2023-09-26 05:41:52] {NativeCodeLoader} WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import getenv\n",
    "import dotenv\n",
    "\n",
    "dotenv.find_dotenv()\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-26 05:41:56] {SparkSession} WARN: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"spark://spark-master:7077\")\n",
    "        .appName('test-app')\n",
    "        .config(\n",
    "            map={\n",
    "                \"spark.hadoop.fs.s3a.access.key\": getenv(\"S3_ACCESS_KEY_ID\"),\n",
    "                \"spark.hadoop.fs.s3a.secret.key\": getenv(\"S3_SECRET_ACCESS_KEY\"),\n",
    "                \"spark.hadoop.fs.s3a.endpoint\": getenv(\"S3_ENDPOINT_URL\"),\n",
    "                \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "            }\n",
    "        )\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVER = (\n",
    "    f\"{getenv('YC_KAFKA_BROKER_HOST')}:{getenv('YC_KAFKA_BROKER_PORT')}\"\n",
    ")\n",
    "USERNAME = getenv(\"YC_KAFKA_USERNAME\")\n",
    "PASSWORD = getenv(\"YC_KAFKA_PASSWORD\")\n",
    "CERTIFICATE_PATH = f'{getenv(\"APP_PATH\")}/CA.pem'\n",
    "\n",
    "if not all([BOOTSTRAP_SERVER, USERNAME, PASSWORD]):\n",
    "    raise ValueError(\n",
    "        \"Required environment varialbes not set. See .env.template for more details\"\n",
    "    )\n",
    "log.info(\n",
    "    f\"Reading stream with given config:\\n {pformat(object=config, underscore_numbers=True)}\"\n",
    ")\n",
    "\n",
    "options = {\n",
    "    \"kafka.bootstrap.servers\": BOOTSTRAP_SERVER,\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"subscribe\": config[\"topic\"],\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"SCRAM-SHA-512\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{USERNAME}\" password=\"{PASSWORD}\";',\n",
    "    \"kafka.ssl.truststore.type\": \"PEM\",\n",
    "    \"kafka.ssl.truststore.location\": CERTIFICATE_PATH,\n",
    "    \"maxOffsetsPerTrigger\": config[\"trigger\"][\"offsets-per-trigger\"],\n",
    "}\n",
    "\n",
    "log.info(\n",
    "    f\"Subscribe to {config['topic']} kafka topic. Will consume {config['trigger']['offsets-per-trigger']} offsets per one trigger\"\n",
    ")\n",
    "\n",
    "frame = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .options(**options)\n",
    "    .load()\n",
    "    .select(\n",
    "        F.col(\"value\").cast(T.StringType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"value\",\n",
    "        F.from_json(\n",
    "            col=F.col(\"value\"),\n",
    "            schema=T.StructType(\n",
    "                [\n",
    "                    T.StructField(\"object_id\", T.StringType(), True),\n",
    "                    T.StructField(\"object_type\", T.StringType(), True),\n",
    "                    T.StructField(\"sent_dttm\", T.StringType(), True),\n",
    "                    T.StructField(\"payload\", T.StringType(), True),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    .withColumns(\n",
    "        dict(\n",
    "            object_id=F.col(\"value.object_id\"),\n",
    "            object_type=F.col(\"value.object_type\"),\n",
    "            sent_dttm=F.col(\"value.sent_dttm\"),\n",
    "            payload=F.col(\"value.payload\"),\n",
    "        )\n",
    "    )\n",
    "    .withColumns(\n",
    "        dict(\n",
    "            sent_dttm=F.to_timestamp(\n",
    "                F.regexp_replace(F.col(\"sent_dttm\"), \"T\", \" \"),\n",
    "                r\"yyyy-MM-dd HH:mm:ss\",\n",
    "            ),\n",
    "            object_type=F.lower(F.col(\"object_type\")),\n",
    "            trigger_dttm=F.lit(datetime.now()),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
